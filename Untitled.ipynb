{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1547750-f8a7-46d2-878d-71df868d1c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from requests) (2024.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e160fbde-f20f-42b8-8a4f-d9d3dc880a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from beautifulsoup4) (2.5)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0e2e2dc-fbee-4064-abc2-b24598e3dc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "662efced-769e-42d4-97e8-2b4f5eaebfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting selenium\n",
      "  Downloading selenium-4.23.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.2.1)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Downloading trio-0.26.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.11.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (2024.2.2)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from selenium) (4.12.0)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (23.2.0)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (3.6)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Downloading outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from trio~=0.17->selenium) (1.3.1)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Downloading wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n",
      "Downloading selenium-4.23.1-py3-none-any.whl (9.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.4/9.4 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio-0.26.0-py3-none-any.whl (475 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.7/475.7 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading trio_websocket-0.11.1-py3-none-any.whl (17 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Downloading outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Installing collected packages: sortedcontainers, wsproto, websocket-client, pysocks, outcome, trio, trio-websocket, selenium\n",
      "  Attempting uninstall: websocket-client\n",
      "    Found existing installation: websocket-client 1.7.0\n",
      "    Uninstalling websocket-client-1.7.0:\n",
      "      Successfully uninstalled websocket-client-1.7.0\n",
      "Successfully installed outcome-1.3.0.post0 pysocks-1.7.1 selenium-4.23.1 sortedcontainers-2.4.0 trio-0.26.0 trio-websocket-0.11.1 websocket-client-1.8.0 wsproto-1.2.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9497206b-9e4e-4dae-a5c7-8f4bbbf057c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57dd96b5-1943-4810-b12e-37b92b321c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "# Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()  # Make sure you have the ChromeDriver installed\n",
    "driver.get('https://www.reddit.com/r/aggies/')\n",
    "\n",
    "# Extract the page source after it has fully loaded\n",
    "html_content = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "# Continue with your BeautifulSoup parsing\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "posts = soup.find_all('div', {'data-testid': 'post-container'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "61da5f16-8590-45dc-883c-6d2821b89a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d3d74-1b47-4f3e-bb1c-cfe5d6c86720",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9737b-1100-4f90-8adf-d8f1c9fd7e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "70017746-05bb-403c-83d2-905aa71fe9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Step 1: Set up the Selenium WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://www.reddit.com/r/aggies/')\n",
    "\n",
    "# Step 2: Scroll to the bottom of the page to load all images\n",
    "SCROLL_PAUSE_TIME = 5\n",
    "scroll_amount = 400\n",
    "for i in range(20):  # Adjust this range for deeper scrolling if needed\n",
    "    driver.execute_script(f\"window.scrollBy(0, {scroll_amount});\")\n",
    "    time.sleep(SCROLL_PAUSE_TIME)\n",
    "\n",
    "# Step 3: Execute JavaScript to force all images to load\n",
    "driver.execute_script(\"\"\"\n",
    "    let images = document.querySelectorAll('img');\n",
    "    images.forEach(img => {\n",
    "        if (img.getAttribute('loading') === 'lazy') {\n",
    "            img.scrollIntoView();\n",
    "            img.setAttribute('src', img.getAttribute('srcset') ? img.getAttribute('srcset').split(', ')[0].split(' ')[0] : img.getAttribute('src'));\n",
    "        }\n",
    "    });\n",
    "\"\"\")\n",
    "\n",
    "# Final wait to ensure everything has loaded\n",
    "time.sleep(10)\n",
    "\n",
    "# Step 4: Extract the page source after ensuring images are loaded\n",
    "html_content = driver.page_source\n",
    "driver.quit()\n",
    "\n",
    "# Step 5: Use BeautifulSoup to parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Step 6: Extract image URLs from each post\n",
    "posts = soup.find_all('div', {'data-testid': 'post-container'})\n",
    "for i, post in enumerate(posts):\n",
    "    title = post.find('h3').text if post.find('h3') else \"No title\"\n",
    "    score = post.find('div', {'class': '_1rZYMD_4xY3gRcSS3p8ODO'}).text if post.find('div', {'class': '_1rZYMD_4xY3gRcSS3p8ODO'}) else \"No score\"\n",
    "    post_link = post.find('a', {'data-click-id': 'comments'})['href'] if post.find('a', {'data-click-id': 'comments'}) else \"No link\"\n",
    "\n",
    "    # Find images within the post\n",
    "    image_tags = post.find_all('img')\n",
    "    for img in image_tags:\n",
    "        src = img.get('src')\n",
    "        srcset = img.get('srcset')\n",
    "        print(f\"Image src: {src}\")\n",
    "        print(f\"Image srcset: {srcset}\")\n",
    "\n",
    "    print(f\"Post {i+1}:\")\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Score: {score}\")\n",
    "    print(f\"Link: {'https://www.reddit.com' + post_link}\")\n",
    "    print(\"-\" * 80)  # Separator for readability\n",
    "\n",
    "# Optional: Save the data to a file if needed\n",
    "output_file = 'reddit_aggies_scraped_data.txt'\n",
    "with open(output_file, 'w') as f:\n",
    "    for post in posts:\n",
    "        title = post.find('h3').text if post.find('h3') else \"No title\"\n",
    "        score = post.find('div', {'class': '_1rZYMD_4xY3gRcSS3p8ODO'}).text if post.find('div', {'class': '_1rZYMD_4xY3gRcSS3p8ODO'}) else \"No score\"\n",
    "        post_link = post.find('a', {'data-click-id': 'comments'})['href'] if post.find('a', {'data-click-id': 'comments'}) else \"No link\"\n",
    "        \n",
    "        image_tags = post.find_all('img')\n",
    "        for img in image_tags:\n",
    "            src = img.get('src')\n",
    "            srcset = img.get('srcset')\n",
    "            f.write(f\"Image src: {src}\\n\")\n",
    "            f.write(f\"Image srcset: {srcset}\\n\")\n",
    "        \n",
    "        f.write(f\"Title: {title}\\n\")\n",
    "        f.write(f\"Score: {score}\\n\")\n",
    "        f.write(f\"Link: {'https://www.reddit.com' + post_link}\\n\")\n",
    "        f.write(\"-\" * 80 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0783a66-43a8-4d5b-8029-99f5dd318c03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
